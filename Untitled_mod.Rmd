---
title: "Untitled"
output: pdf_document
---

---
title: "Breast Cancer Diagnosis"
output: pdf_document
---

```{r setup, echo = FALSE, warnings = FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(viridis)
library(corrplot)
library(pROC)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Data import
```{r}
# --- Import data ---
breast_cancer = 
  read_csv("./data/breast-cancer.csv") %>% 
  select(-c(1, 33)) %>% 
  mutate(diagnosis = 
           as.numeric(as.factor(recode(diagnosis, `M` = 1, `B` = 0))) - 1) %>% 
  mutate_each_(funs(scale(.)), c(2:31))

# --- Data partition ---
# Newton-Raphson Algorithm
pred_1 <- as.tibble(breast_cancer[2:11])
bc_scale1 <- as.matrix(cbind(intercept = rep(1, nrow(breast_cancer)), 
                             pred_1, outcome = breast_cancer$diagnosis))
bc_scale1_x <- bc_scale1[, -12]
bc_scale1_y <- bc_scale1[, 12]
  
# Coordinate-wise descending Algorithm
pred_2 <- as.tibble(breast_cancer[2:31])
bc_scale2 <- as.matrix(cbind(intercept = rep(1, nrow(breast_cancer)), 
                             pred_2, outcome = breast_cancer$diagnosis))
bc_scale2_x <- bc_scale2[, -32]
bc_scale2_y <- bc_scale2[, 32]

# --- Split train vs. test? ---
```


## Add EDA
```{r}
# Correlation plot
corrplot::corrplot(cor(breast_cancer[2:31]), type = "full",
                   tl.cex = .7, tl.col = "black")
```


## Modified Newton-Raphson method
```{r}
# Compute likelihood function, gradient, and Hessian matrix
compute_stat <- function(dat, betavec){
  x <- dat # include intercept
  y <- breast_cancer$diagnosis
  u <- x %*% betavec # n * 1 vector
  pi <- exp(u) / (1 + exp(u))
  # Log-likelihood
  loglik <- sum(y * u - log(1 + exp(u)))
  # Gradient -- (p + 1) * 1 vector
  grad <- t(x) %*% (y - pi)
  # Hessian -- (p + 1) * (p + 1) matrix
  hess <- -t(x) %*% diag(c(pi * (1 - pi))) %*% x
  
  return(list(loglik = loglik, grad = grad, hess = hess))
}

# Modified Mewton-Raphson method
NewtonRaphson <- function(dat, func, start, tol = 1e-15, maxier = 200) {
  i <- 0 # Iteration indicator
  cur <- start # Current position / beta's
  
  # Computate log-likelihood, gradient, and Hessian matrix
  stuff <- func(dat, cur)
  res <- c(0, stuff$loglik, cur) # Store results
  prevloglik <- -Inf # Ensure iterations
  
  while (i < maxier && abs(stuff$loglik - prevloglik) > tol) {
    # --- Base case ---
    i <- i + 1
    step <- 1 # Original step size
    prevloglik <- stuff$loglik
    prev <- cur
    cur <- prev - solve(stuff$hess) %*% stuff$grad
    
    # Step halving and re-direction
    while (func(dat, cur)$loglik < stuff$loglik) {
      # Check if all eigenvalues are negative
      # IF NOT, redirection
      if (!all(eigen(stuff$hess)$values < 0)) {
        gamma <- max(eigen(stuff$hess)$values)
        new_hess <- stuff$hess - gamma * diag(nrow = dim(dat)[1])
        cur <- prev - solve(new_hess) %*% stuff$grad
      }
      # Reduce step size by half
      else {
        step <- step / 2
        cur <- prev - step * solve(stuff$hess) %*% stuff$grad
      }
    }
    stuff <- func(dat, cur)
    
    res <- rbind(res, c(i, stuff$loglik, cur))
  }
  return(res)
}
```


## NR results examples
```{r}
# NewtonRaphson(bc_scale1_x, compute_stat, rep(1, 11))
# NewtonRaphson(bc_scale1_x, compute_stat, rep(10, 11))

# The result of complete number of predictors is not ideal
# due to some potential collinearity
```


## Compare with `glm` reults
```{r}
# Fit a logistic model
glm_fit <- glm(diagnosis ~., family = binomial(link = "logit"), data = breast_cancer[, 1:11])

# Comparison
NR_result <- NewtonRaphson(bc_scale1_x, compute_stat, rep(10, 11))
round(tail(NR_result, 1)[, 3:13], 3) == round(as.vector(glm_fit$coefficients), 3) # return all TRUE
```


## Soft threshold and coor-wise Lasso
```{r}
# Soft Threshold function
sf <- function(beta, lambda) {
  beta <- ifelse(lambda < abs(beta), 
                 ifelse(beta > 0, beta - lambda, beta + lambda), 0)
  return(beta)
}


# Coordinate-wise LASSO with fixed lambda
y <- breast_cancer$diagnosis

cd_lasso <- function(dat, y, betavec, lambda, maxier = 2000, tol = 1e-10) {
  x <- dat # n * (p + 1) matrix, standardized
  i <- 0
  loglik <- 0
  prevloglik <- -Inf # ensure iterations
  res <- c(0, loglik, betavec)
  
  while (i < maxier && abs(loglik - prevloglik) > tol) {
    i <- i + 1
    prevloglik <- loglik
    
    for (j in 1:length(betavec)) {
      u <- x %*% betavec
      pi <- exp(u) / (1 + exp(u))
      # working weights
      w <- pi * (1 - pi)
      w <- ifelse(w < 1e-5, 1e-5, w)
      # working response
      z <- x %*% betavec + (y - pi) / w  
      z_dej <- x[, -j] %*% betavec[-j]
      betavec[j] <- 
        sf(sum(w * x[, j] * (z - z_dej)), lambda) / (sum(w * x[, j]^2))
    }
    
    loglik <- 
      sum(w * (z - x %*% betavec)^2) / (2 * dim(x)[1]) + lambda * sum(abs(betavec))
    
    res <- rbind(res, c(i, loglik, betavec)) 
  }
  
  return(res)
}
```


### Try...
```{r}
a <- cd_lasso(bc_scale1_x, y, rep(1,11), lambda = 10)
```


## path-wise coordinate-wise Lasso
```{r}
# --- To be modified ---
pathwise_cd_lasso <- function(dat, betavec, lambda, maxier = 2000, tol = 1e-10) {
  sort(lambda, decreasing = TRUE)
  pathwise_res <- NA
  
  for (j in 1:length(lambda)) {
    cd_results <- cd_lasso(dat, y = breast_cancer$diagnosis, betavec, lambda[j], maxier, tol)
    
    betavec <- cd_results[dim(cd_results)[1], -c(1, 2)]
    loglik <- cd_results[dim(cd_results)[1], 2]
    pathwise_res <- rbind(pathwise_res, 
                          c(j, lambda[j], loglik, betavec))
    
  }
    
  return(pathwise_res)
}
```


### Try...
```{r}
b <- pathwise_cd_lasso(bc_scale2_x, rep(1, 31), lambda = seq(5, 0, length = 100))
```


## 5-fold CV
```{r}
# create data partition
n = 500
x1 = rnorm(n, mean = 1)
x2 = rnorm(n, mean = 1)
eps = rnorm(n, sd = 0.5)
y = sin(x1) + (x2)^2 + eps
a = data.frame(y = y, x1  = x1)
#cbind(a, x2)
indexTrain = sample(nrow(a), 400, replace = F)
a[indexTrain, ]
a[-indexTrain, ]
#cut(seq_along(a), n = 5, lables = F)
fold = cut(seq(1,nrow(a)),breaks = 5,labels = FALSE)
which(fold == 3, arr.ind = T)
```

```{r}
cv = function(raw_data, lambda_vec, kfolds = 5, betavec){
  fold_index = cut(seq(1,nrow(raw_data)),breaks = kfolds, labels = FALSE)  # 1:length(data)
  auc = vector()
  #se = vector()
  for (i in length(lambda_vec)){
    # shuffle data
    raw_data = raw_data[sample(nrow(raw_data)), ]
    fold_auc = vector()
    for (j in 1:kfolds){
      validation = raw_data[fold_index == j, ]
      train_x = raw_data[fold_index != j, -ncol(raw_data)]
      train_y <- raw_data[fold_index != j, ncol(raw_data)]
      
      cd_results = cd_lasso(train_x, train_y, betavec, lambda_vec[i])
      curbetavec = cd_results[dim(cd_results)[1], -c(1, 2)]
      
      u = validation[,-ncol(validation)] %*% curbetavec
      prob = exp(u)/(1 + exp(u))
      
     fold_auc[j] = auc(prob, as.factor(validation[,ncol(validation)]))
    }
    # auc fill in
    auc[i] = mean(fold_auc)
    
  }
  return(auc)
}
#fold_index = cut(seq(1,nrow(bc_scale1)),breaks = 5, labels = F)
#c = bc_scale1[sample(nrow(bc_scale1)), ]
#training = c[fold_index != 1, -ncol(c)]
#cd_lasso(training, betavec = rep(1, 11), lambda = 1)
#typeof(c[fold_index == 2, ])
#bc_scale1[,-ncol(bc_scale1)]
cv(bc_scale1, lambda_vec = seq(5, 0, length = 100), betavec = rep(1, 11))
auc(0.6, 0.8)
```



